import alf.algorithms.hypernetwork_layer_algorithm

# dataset config
create_dataset.dataset_name='mnist_inlier'
#create_dataset.dataset_name='mnist'
create_dataset.train_batch_size=50
create_dataset.test_batch_size=100

# Lenet for MNIST
CONV_LAYER_PARAMS = ((6, 5, 1, 2, 2), (16, 5, 1, 0, 2), (120, 5, 1))
FC_LAYER_PARAMS = ((84, True), )
# HIDDEN_LAYERS = (512,1024)
HIDDEN_LAYERS = (256,512,1024)
# HIDDEN_LAYERS = (800,1200,1500)

# HIDDEN_LAYERS = (500,1000)

hypernet/Adam.lr=1e-5
hypernet/Adam.weight_decay=0 # 1e-4
hypernet_pinverse/Adam.lr=1e-4

# algorithm config
HyperNetwork.conv_layer_params = %CONV_LAYER_PARAMS
HyperNetwork.fc_layer_params=%FC_LAYER_PARAMS
HyperNetwork.hidden_layers=%HIDDEN_LAYERS
HyperNetwork.noise_dim=256
HyperNetwork.num_particles=10

HyperNetwork.use_fc_bn=True
HyperNetwork.parameterization = 'network'
HyperNetwork.par_vi = 'svgd3'
HyperNetwork.loss_type = 'classification'
HyperNetwork.function_vi = False
HyperNetwork.functional_gradient = 'rkhs'
HyperNetwork.force_fullrank = False
HyperNetwork.use_jac_regularization = False
HyperNetwork.pinverse_solve_iters = 1
HyperNetwork.pinverse_hidden_size = 256 # 512
HyperNetwork.entropy_regularization = 1e-5
HyperNetwork.function_bs = 50
HyperNetwork.optimizer=@hypernet/Adam()
HyperNetwork.pinverse_optimizer=@hypernet_pinverse/Adam()
HyperNetwork.logging_training=True
HyperNetwork.logging_evaluate=True

import alf.networks.param_networks
ParamConvNet.use_bias=True

import alf.algorithms.hypernetwork_layer_generator
ParamLayers.use_bias=True

# training config
TrainerConfig.algorithm_ctor=@HyperNetwork
TrainerConfig.num_epochs=100
TrainerConfig.num_checkpoints=1
TrainerConfig.evaluate=True
TrainerConfig.eval_uncertainty=True
TrainerConfig.eval_interval=1
TrainerConfig.summary_interval=1
TrainerConfig.debug_summaries=True
TrainerConfig.summarize_grads_and_vars=True
TrainerConfig.outlier_dataset='mnist_outlier'
TrainerConfig.random_seed=None
